{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06336b06-2112-47b9-8236-f06295493a89",
   "metadata": {},
   "source": [
    "# **Ujian Akhir Semester - Pengolahan Bahasa Alami**\n",
    "\n",
    "## Anggota Kelompok:\n",
    "- **Kevin Philips Tanamas** (220711789)  \n",
    "- **Richard Angelico** (220711747)\n",
    "- **Anthony Alvin Nathaniel** (220711773)\n",
    "- **Nicholas Raymond Thosimaru** (220712111)\n",
    "- **Maria** (220711969)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200e11b-1806-4bca-abd3-6b004f206437",
   "metadata": {},
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a508b82b-34f9-4db8-a939-dfdf73faf7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "import gc\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seqeval.scheme import BILOU\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, GRU, Dense, TimeDistributed, Dropout\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc946f-13b3-4ad2-a55e-aa26aa1ee59b",
   "metadata": {},
   "source": [
    "Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0d939b-d3ef-4389-b062-8e5788bf1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CPU usage for memory-intensive operations\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Use CPU for data preparation\n",
    "\n",
    "# Data loading function\n",
    "def load_data(filename):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == '':\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    word, tag = parts\n",
    "                    sentence.append(word)\n",
    "                    label.append(tag)\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def simple_rnn_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    SimpleRNN model for NER - fastest training, but typically lower accuracy\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of RNN units\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Optional projection to reduce dimensions\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # SimpleRNN layer - fastest but less capable for sequence modeling\n",
    "    x = tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True)(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer with softmax activation for tag prediction\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e8792-f273-4de0-bdf4-498e802239b0",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8517eb9-026d-4ca1-ba75-e10ce97b6f23",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_betawi.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m                 outfile.write(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Tambahkan baris kosong\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Jalankan ini sekali untuk membuat file baru yang sudah dipisah\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43minsert_blank_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_betawi.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_betawi_split.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36minsert_blank_lines\u001b[39m\u001b[34m(input_path, output_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minsert_blank_lines\u001b[39m(input_path, output_path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m infile, \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m infile:\n\u001b[32m      4\u001b[39m             outfile.write(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset_betawi.txt'"
     ]
    }
   ],
   "source": [
    "def insert_blank_lines(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line)\n",
    "            if line.strip().endswith(('.-O', '!-O', '?-O')):\n",
    "                outfile.write('\\n')  # Tambahkan baris kosong\n",
    "\n",
    "# Jalankan ini sekali untuk membuat file baru yang sudah dipisah\n",
    "insert_blank_lines(\"dataset_betawi.txt\", \"dataset_betawi_split.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label_seq = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Baris kosong = akhir kalimat\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label_seq)\n",
    "                    sentence = []\n",
    "                    label_seq = []\n",
    "                continue\n",
    "\n",
    "            # Hapus nomor dan titik dua di depan (contoh: \"12: Ane-O\")\n",
    "            if \":\" in line:\n",
    "                line = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "            if \"-\" not in line:\n",
    "                continue\n",
    "\n",
    "            token, tag = line.rsplit(\"-\", 1)\n",
    "            sentence.append(token)\n",
    "            label_seq.append(tag)\n",
    "\n",
    "        # Tambahkan kalimat terakhir jika ada\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label_seq)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset_betawi_split.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for i, line in enumerate(lines[:20]):\n",
    "    print(f\"{i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "full_file = \"./dataset_betawi_split.txt\"\n",
    "sentences, labels = load_data(full_file)\n",
    "\n",
    "# Check if data loaded\n",
    "print(f\"Loaded {len(sentences)} samples.\")\n",
    "\n",
    "# Split into train (70%) and temp (30%)\n",
    "train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into validation (15%) and test (15%)\n",
    "valid_sentences, test_sentences, valid_labels, test_labels = train_test_split(\n",
    "    temp_sentences, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.DataFrame({'tokens': train_sentences, 'tags': train_labels})\n",
    "valid_df = pd.DataFrame({'tokens': valid_sentences, 'tags': valid_labels})\n",
    "test_df = pd.DataFrame({'tokens': test_sentences, 'tags': test_labels})\n",
    "\n",
    "# Print sizes\n",
    "print(f\"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a69899-1bf6-4180-9a27-219b2bd2c91f",
   "metadata": {},
   "source": [
    "Data Checking (Liat Semua Tags) & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf249be-fdec-46cf-a8da-e01a5fb7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all tags (flattened) from training labels\n",
    "all_tags_flat = [tag for seq in train_labels for tag in seq]\n",
    "\n",
    "print(f\"Total tags in training: {len(all_tags_flat)}\")\n",
    "\n",
    "print(\"Tag distribution in training set:\")\n",
    "tag_counts = pd.Series(all_tags_flat).value_counts()\n",
    "print(tag_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=tag_counts.values, y=tag_counts.index, palette=\"viridis\")\n",
    "plt.title(\"Distribusi Tag di Dataset\")\n",
    "plt.xlabel(\"Jumlah Token\")\n",
    "plt.ylabel(\"Tag\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = [len(seq) for seq in train_sentences]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(train_lengths, bins=20, kde=True)\n",
    "plt.title(\"Distribusi Panjang Kalimat di Dataset\")\n",
    "plt.xlabel(\"Jumlah Token per Kalimat\")\n",
    "plt.ylabel(\"Jumlah Kalimat\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [tok for seq in train_sentences for tok in seq]\n",
    "token_counts = Counter(all_tokens)\n",
    "common_tokens = pd.Series(dict(token_counts.most_common(10)))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=common_tokens.values, y=common_tokens.index, palette=\"mako\")\n",
    "plt.title(\"10 Token Paling Sering Muncul di Dataset\")\n",
    "plt.xlabel(\"Frekuensi\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counts = pd.Series({\n",
    "    \"Train\": len(train_df),\n",
    "    \"Valid\": len(valid_df),\n",
    "    \"Test\": len(test_df)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=split_counts.index, y=split_counts.values, palette=\"deep\")\n",
    "plt.title(\"Jumlah Kalimat per Split Dataset\")\n",
    "plt.ylabel(\"Jumlah Kalimat\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9877d0-4181-45d8-a7d7-c05891c1128b",
   "metadata": {},
   "source": [
    "Load FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a454dd-b280-4928-ac65-2eab35e29409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek dan unduh FastText model jika belum tersedia\n",
    "if not os.path.exists('./cc.id.300.bin'):\n",
    "    print(\"FastText model not found. Downloading model (this may take a while)...\")\n",
    "    os.system(\"wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\")\n",
    "    os.system(\"gunzip cc.id.300.bin.gz\")\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Load FastText model\n",
    "print(\"Loading FastText model...\")\n",
    "ft = fasttext.load_model('./cc.id.300.bin')\n",
    "print(\"FastText model loaded successfully.\")\n",
    "\n",
    "# Set embedding size dan panjang maksimum urutan\n",
    "emb_size = 300\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ecc3a",
   "metadata": {},
   "source": [
    "Tag Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c10e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Buat mapping tag ke ID\n",
    "all_tags = list(set([tag for seq in train_df['tags'] for tag in seq]))\n",
    "tag2id = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "num_tags = len(tag2id)\n",
    "\n",
    "# Encode dan pad tag sequence\n",
    "def encode_tags(tag_seq, tag2id, max_len):\n",
    "    tag_ids = [tag2id.get(tag, tag2id['O']) for tag in tag_seq]\n",
    "    return pad_sequences([tag_ids], maxlen=max_seq_len, padding='post', value=tag2id['O'])[0]\n",
    "\n",
    "train_df['tag_ids'] = train_df['tags'].apply(lambda tags: encode_tags(tags, tag2id, max_seq_len))\n",
    "valid_df['tag_ids'] = valid_df['tags'].apply(lambda tags: encode_tags(tags, tag2id, max_seq_len))\n",
    "test_df['tag_ids'] = test_df['tags'].apply(lambda tags: encode_tags(tags, tag2id, max_seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a3f61-cb23-4fad-960e-1daf6553dd58",
   "metadata": {},
   "source": [
    "Word Embeddings Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468227d-66ec-4b46-8c00-b3108ca3e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi untuk menghasilkan word embeddings dari token\n",
    "def get_tok_emb(tokens):\n",
    "    emb = np.zeros((max_seq_len, emb_size))\n",
    "    for i, word in enumerate(tokens[:max_seq_len]):\n",
    "        emb[i] = ft.get_word_vector(word)\n",
    "    return emb\n",
    "\n",
    "# Proses pembuatan embeddings untuk setiap token dalam dataset\n",
    "print(\"Creating word embeddings...\")\n",
    "train_df['tokens_embedding'] = train_df['tokens'].progress_apply(get_tok_emb)\n",
    "valid_df['tokens_embedding'] = valid_df['tokens'].progress_apply(get_tok_emb)\n",
    "test_df['tokens_embedding'] = test_df['tokens'].progress_apply(get_tok_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat mapping untuk tag ke indeks dan sebaliknya\n",
    "print(\"Creating tag mapping...\")\n",
    "\n",
    "# Ambil semua tag unik dari data latih\n",
    "all_tags = sorted(set(tag for seq in train_df['tags'] for tag in seq))\n",
    "print(f\"Number of unique tags: {len(all_tags)}\")\n",
    "\n",
    "# Buat mapping dari tag ke indeks, dengan indeks 0 untuk padding\n",
    "tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "tag2index['PAD'] = 0  # PAD token at index 0\n",
    "\n",
    "# Buat mapping sebaliknya dari indeks ke tag\n",
    "index2tag = {idx: tag for tag, idx in tag2index.items()}\n",
    "\n",
    "# Tampilkan beberapa mapping untuk verifikasi\n",
    "print(\"Tag to index mapping (sample):\")\n",
    "for i, (tag, idx) in enumerate(tag2index.items()):\n",
    "    if i < 10:  # Tampilkan 10 pertama\n",
    "        print(f\"{tag} -> {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134c876",
   "metadata": {},
   "source": [
    "Dimensionality Reduction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua embedding untuk fitting PCA\n",
    "print(\"Fitting PCA...\")\n",
    "all_embeddings = np.vstack(train_df['tokens_embedding'].values)\n",
    "pca_components = 50\n",
    "pca = PCA(n_components=pca_components)\n",
    "pca.fit(all_embeddings)\n",
    "\n",
    "# Transform setiap embedding sequence\n",
    "def reduce_dims(emb_matrix):\n",
    "    return pca.transform(emb_matrix)\n",
    "\n",
    "train_df['tokens_embedding_pca'] = train_df['tokens_embedding'].progress_apply(reduce_dims)\n",
    "valid_df['tokens_embedding_pca'] = valid_df['tokens_embedding'].progress_apply(reduce_dims)\n",
    "test_df['tokens_embedding_pca'] = test_df['tokens_embedding'].progress_apply(reduce_dims)\n",
    "\n",
    "# Update embedding size ke dimensi baru\n",
    "emb_size = pca_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fe43a",
   "metadata": {},
   "source": [
    "Optimasi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(train_df['tokens_embedding_pca'].values)\n",
    "y_train = np.stack(train_df['tag_ids'].values)\n",
    "\n",
    "X_valid = np.stack(valid_df['tokens_embedding_pca'].values)\n",
    "y_valid = np.stack(valid_df['tag_ids'].values)\n",
    "\n",
    "X_test = np.stack(test_df['tokens_embedding_pca'].values)\n",
    "y_test = np.stack(test_df['tag_ids'].values)\n",
    "\n",
    "# Expand label dims: (batch_size, timesteps, 1)\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_valid = np.expand_dims(y_valid, -1)\n",
    "y_test = np.expand_dims(y_test, -1)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6dee39-65db-4304-a700-9d94516b4318",
   "metadata": {},
   "source": [
    "Bilstm Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa4739-f30f-4e75-aca3-a41f36ac0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for NER - highest accuracy but slowest training\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of LSTM units per direction\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Projection layer to reduce dimensions\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # Bidirectional LSTM - processes sequences in both directions\n",
    "    # Higher accuracy for context-dependent tasks like NER\n",
    "    x = Bidirectional(\n",
    "        LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.0,\n",
    "            implementation=2  # potentially faster but less stable\n",
    "        )\n",
    "    )(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def bigru_model(max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    \"\"\"\n",
    "    Bidirectional GRU model - good balance between speed and accuracy\n",
    "\n",
    "    Args:\n",
    "        max_len_seq: Maximum sequence length\n",
    "        emb_size: Embedding size\n",
    "        rnn_units: Number of GRU units per direction\n",
    "        num_tags: Number of output tags\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_len_seq, emb_size), dtype='float32')\n",
    "\n",
    "    # Projection layer\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "\n",
    "    # Bidirectional GRU - faster than LSTM with similar capabilities\n",
    "    x = Bidirectional(\n",
    "        GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_dropout=0.0,\n",
    "            reset_after=True  # modern GRU implementation\n",
    "        )\n",
    "    )(x)\n",
    "\n",
    "    # Dropout for regularization\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(num_tags, activation='softmax'))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_dataset(X, y, batch_size=16, shuffle=False, buffer_size=1000):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea07c0",
   "metadata": {},
   "source": [
    "Model Configuration â€“ Arsitektur BilSTM (Bisa dipilih sesuai kebutuhan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ee602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat dataset untuk training dan validasi\n",
    "batch_size = 16\n",
    "train_dataset = create_optimized_dataset(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = create_optimized_dataset(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "# Konfigurasi callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-4)\n",
    "]\n",
    "\n",
    "def get_model(selected_model, max_len_seq, emb_size, rnn_units, num_tags, lr=0.001):\n",
    "    if selected_model == 'BiLSTM':\n",
    "        return bilstm_model(max_len_seq, emb_size, rnn_units, num_tags, lr)\n",
    "    elif selected_model == 'BiGRU':\n",
    "        return bigru_model(max_len_seq, emb_size, rnn_units, num_tags, lr)\n",
    "    elif selected_model == 'SimpleRNN':\n",
    "        return simple_rnn_model(max_len_seq, emb_size, rnn_units, num_tags, lr)\n",
    "    else:\n",
    "        raise ValueError(\"Model tidak dikenali. Pilih dari: 'BiLSTM', 'BiGRU', 'SimpleRNN'\")\n",
    "\n",
    "# Pilihan model yang tersedia\n",
    "selected_model = 'BiLSTM'  # Bisa diganti menjadi 'BiGRU' atau 'SimpleRNN'\n",
    "\n",
    "# Dapatkan model berdasarkan pilihan\n",
    "model = get_model(\n",
    "    selected_model=selected_model,\n",
    "    max_len_seq=X_train.shape[1],\n",
    "    emb_size=X_train.shape[2],\n",
    "    rnn_units=64,\n",
    "    num_tags=num_tags,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Tampilkan ringkasan model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087dfdd",
   "metadata": {},
   "source": [
    "Training Model dengan EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e64b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08eda0",
   "metadata": {},
   "source": [
    "Simpan Metrics Training ke DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simpan history training ke CSV\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f\"history_{selected_model}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0f730",
   "metadata": {},
   "source": [
    "Visualisasi Loss dan Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(f'{selected_model} - Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title(f'{selected_model} - Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{selected_model}_training_plot.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
